{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645946ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "#!pip install ftfy\n",
    "import ftfy\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "#!pip install sparse_dot_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "jobData = pd.read_csv('jobster_data.csv', dtype={'salary': 'str'})\n",
    "industryLookup = pd.read_csv('industry_lookup.csv')\n",
    "\n",
    "print('The jobData shape: %d x %d' % jobData.shape)\n",
    "print('The lookup table shape: %d x %d' % industryLookup.shape)\n",
    "\n",
    "\n",
    "naiveMerge = pd.merge(jobData, industryLookup, left_on = 'company', right_on = 'companyName', how = 'inner')\n",
    "num_rows_with_amonth = jobData['salary'].str.contains('a month').sum()\n",
    "num_rows_with_anhour = jobData['salary'].str.contains('an hour').sum()\n",
    "num_rows_with_aweek = jobData['salary'].str.contains('a week').sum()\n",
    "\n",
    "print(f\"Number of rows with 'a month': {num_rows_with_amonth}\")\n",
    "print(f\"Number of rows with 'an hour': {num_rows_with_anhour}\")\n",
    "print(f\"Number of rows with 'a week': {num_rows_with_aweek}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b76b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "#!pip install ftfy\n",
    "import ftfy\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "#!pip install sparse_dot_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "jobData = pd.read_csv('jobster_data.csv', dtype={'salary': 'str'})\n",
    "industryLookup = pd.read_csv('industry_lookup.csv')\n",
    "\n",
    "print('The jobData shape: %d x %d' % jobData.shape)\n",
    "print('The lookup table shape: %d x %d' % industryLookup.shape)\n",
    "\n",
    "org_names = pd.concat([jobData['company'], industryLookup['companyName']]).unique()\n",
    "print(type(org_names))\n",
    "print(f'There are {org_names.size} companies.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b653c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(string, n = 3):\n",
    "    string = str(string)\n",
    "    string = ftfy.fix_text(string) # fix test\n",
    "    string = string.encode(\"ascii\", errors = \"ignore\").decode()  # remove non ascii chars\n",
    "    string = string.lower()\n",
    "    chars_to_remove = [\")\",\"(\",\".\",\"|\",\"[\",\"]\",\"{\",\"}\",\"'\"]\n",
    "    rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "    string = re.sub(rx, '', string)\n",
    "    string = string.replace('&', 'and')\n",
    "    string = string.replace(',', ' ')\n",
    "    string = string.replace('-', ' ')\n",
    "    string = string.title() # normalize case - capitals at start of each word\n",
    "    string = re.sub(' +', ' ',string).strip() #get rid of multiple spaces and replace with a single\n",
    "    string = ' '+ string + ' ' # pad names for ngrams\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'',string) # remove any hyphens, commas, periods, or whitespace\n",
    "    ngrams = zip(*[string[i:] for i in range(n)]) # create n-grams\n",
    "    return [''.join(ngram) for ngram in ngrams] # convert list of n-grams to list of strings\n",
    "\n",
    "\n",
    "ngramTest = ngrams(\"apple\")\n",
    "print(ngramTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977df012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "#!pip install ftfy\n",
    "import ftfy\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "#!pip install sparse_dot_topn\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "jobData = pd.read_csv('jobster_data.csv', dtype={'salary': 'str'})\n",
    "industryLookup = pd.read_csv('industry_lookup.csv')\n",
    "\n",
    "print('The jobData shape: %d x %d' % jobData.shape)\n",
    "print('The lookup table shape: %d x %d' % industryLookup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cced2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!pip install ftfy\n",
    "#!pip install sparse_dot_topn\n",
    "\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from matplotlib import style\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "import ftfy\n",
    "\n",
    "# Increase the maximum column width to show full text in dataframes\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Use the \"fivethirtyeight\" style for matplotlib plots\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "jobData = pd.read_csv('jobster_data.csv', dtype={'salary': 'str'})\n",
    "industryLookup = pd.read_csv('industry_lookup.csv')\n",
    "industryLookup = industryLookup.dropna()\n",
    "print('The jobData shape: %d x %d' % jobData.shape)\n",
    "print('The lookup table shape: %d x %d' % industryLookup.shape)\n",
    "\n",
    "\n",
    "def ngrams(string, n = 3):\n",
    "    string = str(string)\n",
    "    string = ftfy.fix_text(string) # fix text\n",
    "    string = string.encode(\"ascii\", errors = \"ignore\").decode()  # remove non ascii chars\n",
    "    string = string.lower()\n",
    "    chars_to_remove = [\")\",\"(\",\".\",\"|\",\"[\",\"]\",\"{\",\"}\",\"'\"]\n",
    "    rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n",
    "    string = re.sub(rx, '', string)\n",
    "    string = string.replace('&', 'and')\n",
    "    string = string.replace(',', ' ')\n",
    "    string = string.replace('-', ' ')\n",
    "    string = string.title() # normalize case - capitals at start of each word\n",
    "    string = re.sub(' +', ' ',string).strip() #get rid of multiple spaces and replace with a single\n",
    "    string = ' '+ string + ' ' # pad names for ngrams\n",
    "    string = re.sub(r'[,-./]|\\sBD',r'',string) # remove any hyphens, commas, periods, or whitespace\n",
    "    ngrams = zip(*[string[i:] for i in range(n)]) # create n-grams\n",
    "    return [''.join(ngram) for ngram in ngrams] # convert list of n-grams to list of strings\n",
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead.\n",
    "    \n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    "    \n",
    "    idx_dtype = np.int32\n",
    "    \n",
    "    nnz_max = M*ntop\n",
    "    \n",
    "    indptr = np.zeros(M+1, dtype = idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    \n",
    "    data = np.zeros(nnz_max, dtype = A.dtype)\n",
    "    \n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype = idx_dtype),\n",
    "        np.asarray(B.indices, dtype = idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "    \n",
    "    return csr_matrix((data, indices, indptr), shape=(M,N))\n",
    "\n",
    "def get_matches_df(sparse_matrix, name_vector, top=None):\n",
    "    non_zeros = sparse_matrix.nonzero()\n",
    "    \n",
    "    sparserows = non_zeros[0]\n",
    "    sparsecols = non_zeros[1]\n",
    "    \n",
    "    if top:\n",
    "        nr_matches = top\n",
    "    else:\n",
    "        nr_matches = sparsecols.size\n",
    "    \n",
    "    left_side = np.empty([nr_matches], dtype = object)\n",
    "    right_side = np.empty([nr_matches], dtype = object)\n",
    "    similarity = np.zeros(nr_matches)\n",
    "    \n",
    "    for index in range(0, nr_matches):\n",
    "        left_side[index] = name_vector[sparserows[index]]\n",
    "        right_side[index] = name_vector[sparsecols[index]]\n",
    "        similarity[index] = sparse_matrix.data[index]\n",
    "        \n",
    "    return pd.DataFrame({'left_side': left_side,\n",
    "                            'right_side': right_side,\n",
    "                            'similarity': similarity})\n",
    "\n",
    "# Define a threshold for similarity score\n",
    "similarity_threshold = 0.90\n",
    "\n",
    "# Combine company name columns from both dataframes\n",
    "org_names = pd.concat([jobData['company'], industryLookup['companyName']]).unique()\n",
    "print(len(org_names))\n",
    "# create a TfidfVectorizer with the ngrams function as the analyzer\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "# Fit the vectorizer to the combined `companyName` strings\n",
    "tf_idf_matrix = vectorizer.fit_transform(org_names)\n",
    "\n",
    "#time cosine similarity function\n",
    "t1 = time.time()\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, similarity_threshold)\n",
    "t = time.time()-t1\n",
    "print(\"SELFTIMED:\", t)\n",
    "\n",
    "    \n",
    "matches_df = get_matches_df(matches, org_names)\n",
    "matches_df = matches_df[matches_df['similarity'] < 0.999999] # Remove all exact matches\n",
    "\n",
    "display(matches_df)\n",
    "\n",
    "# Suppose you want to find the row(s) where 'left_side' equals 'ABC Inc'\n",
    "string_to_find = 'metro'\n",
    "rows_containing_string = matches_df[matches_df['left_side'].str.contains(string_to_find, case=False)]\n",
    "display(rows_containing_string)\n",
    "\n",
    "\n",
    "# name mapping\n",
    "\n",
    "name_mapping = {}\n",
    "\n",
    "for _, row in matches_df.iterrows():\n",
    "    if row['similarity'] > similarity_threshold:\n",
    "        left_name = row['left_side']\n",
    "        right_name = row['right_side']\n",
    "\n",
    "        # Check if both names have already been mapped to a canonical name\n",
    "        if left_name in name_mapping and right_name in name_mapping:\n",
    "            left_canonical = name_mapping[left_name]\n",
    "            right_canonical = name_mapping[right_name]\n",
    "\n",
    "            # If the canonical names for left and right are different, choose one\n",
    "            if left_canonical != right_canonical:\n",
    "                if left_canonical < right_canonical:\n",
    "                    name_mapping[right_canonical] = left_canonical\n",
    "                    #print(f\"Mapping {right_canonical} to {left_canonical}\")\n",
    "                else:\n",
    "                    name_mapping[left_canonical] = right_canonical\n",
    "                    #print(f\"Mapping {left_canonical} to {right_canonical}\")\n",
    "\n",
    "        # Check if only the left name has been mapped to a canonical name\n",
    "        elif left_name in name_mapping:\n",
    "            right_canonical = name_mapping[left_name]\n",
    "            name_mapping[right_name] = right_canonical\n",
    "            #print(f\"Mapping {right_name} to {right_canonical}\")\n",
    "\n",
    "        # Check if only the right name has been mapped to a canonical name\n",
    "        elif right_name in name_mapping:\n",
    "            left_canonical = name_mapping[right_name]\n",
    "            name_mapping[left_name] = left_canonical\n",
    "            #print(f\"Mapping {left_name} to {left_canonical}\")\n",
    "\n",
    "        # If neither name has been mapped yet, create a new canonical name\n",
    "        else:\n",
    "            name_mapping[left_name] = left_name\n",
    "            name_mapping[right_name] = left_name\n",
    "            #print(f\"Mapping {left_name} to {left_name}\")\n",
    "\n",
    "\n",
    "### !!! not working fully as intended, gets the job done !!!            \n",
    "# Replace company names in jobData\n",
    "jobData['company'] = jobData['company'].replace(name_mapping)\n",
    "print(\"Company names in jobData replaced\")\n",
    "\n",
    "# Replace company names in industryLookup\n",
    "industryLookup['companyName'] = industryLookup['companyName'].replace(name_mapping)\n",
    "print(\"Company names in industryLookup replaced\")\n",
    "\n",
    "\n",
    "# join dataset on company names with provided industry lookup table and export to csv\n",
    "# additional csv files to \n",
    "merged_df = pd.merge(jobData,industryLookup, left_on = 'company', right_on = 'companyName')\n",
    "merged_df = merged_df.drop(columns=['companyName'])\n",
    "merged_df.to_csv('mergedData.csv', index = False)\n",
    "\n",
    "# additional csv exports, basically dont show the changes I hoped for - but should not matter\n",
    "# jobData.to_csv('jobDataNamesNormalized.csv', index = False)\n",
    "# industryLookup.to_csv('industryLookupNamesNormalized.csv', index = False)\n",
    "\n",
    "# export all name matches above similarity threshold (0.90) to csv\n",
    "matches_df.to_csv('allMatches.csv', index = False)\n",
    "\n",
    "\n",
    "# dataframe with matches merged with industry lookup table\n",
    "matches_merged = pd.merge(matches_df, industryLookup, left_on = 'right_side', right_on = 'companyName')\n",
    "matches_merged2 = matches_merged.drop_duplicates(subset=['companyName', 'similarity'])\n",
    "matches_merged.to_csv('allMatchesMergers.csv', index = False)\n",
    "matches_merged2.to_csv('allMatchesMergers2.csv', index = False)\n",
    "\n",
    "# drop unusable salary entries\n",
    "# possible workaround is to identify similar entries with salaries provided and apply those to all other similar entries\n",
    "# this workaround still would not address the main issue\n",
    "regex = r'(\\$?-?(?<=\\$)\\d{1,3}(?:[,.]\\d{3})*(?:\\.\\d{1,2})?[Kk]?)'\n",
    "merged_df['salary'] = merged_df['salary'].astype(str) # convert the column to string\n",
    "\n",
    "\n",
    "\n",
    "finalMerge = merged_df[merged_df['salary'].apply(lambda x: re.search(regex, x) is not None)] # filter rows that match the pattern\n",
    "\n",
    "finalMerge.to_csv('final.csv', index = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
